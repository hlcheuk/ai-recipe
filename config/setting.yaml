llms:
  github:
    gpt-4o-mini:
      model_name: gpt-4o-mini
      max_tokens: 4096
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
    gpt-4o:
      model_name: gpt-4o
      max_tokens: 4096
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
  groq:
    llama-3.1-70b:
      model_name: llama-3.1-70b-versatile
      max_tokens: 4096
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
    llama-3-70b:
      model_name: llama3-groq-70b-8192-tool-use-preview
      max_tokens: 1024
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
  yi-01:
    yi-large:
      model_name: yi-large-fc
      max_tokens: 1024
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
      currency: RMB
      input_price: 20
      output_price: 20
    yi-large-turbo:
      model_name: yi-large-turbo
      max_tokens: 1024
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
      currency: RMB
      input_price: 12
      output_price: 12
    yi-spark:
      model_name: yi-spark
      max_tokens: 1024
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
      currency: RMB
      input_price: 1
      output_price: 11
  deepseek:
    deepseek-chat:
      model_name: deepseek-chat
      max_tokens: 1024
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
  google:
    gemini-1.5-pro:
      model_name: gemini-1.5-pro-001
      max_tokens: 1024
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
      project_id: ai-recipe-dev
    gemini-1.5-flash:
      model_name: gemini-1.5-flash-001
      max_tokens: 1024
      frequency_penalty: 0.1  # not in used
      presence_penalty: 0.1  # not in used
      project_id: ai-recipe-dev
chains:
  intent: # heavy
    llm: yi-large
    # llm: llama-3.1-70b
    # llm: gemini-1.5-pro
    # llm: deepseek-chat
    temperature: 0.01
    top_p: 0.95
  chitchat: # light
    llm: yi-large-turbo
    # llm: yi-spark
    # llm: llama-3.1-70b
    # llm: gemini-1.5-flash
    temperature: 0.5
    top_p: 0.95
  cusine: # heavy
    llm: yi-large
    # llm: llama-3.1-70b
    # llm: gemini-1.5-pro
    # llm: deepseek-chat
    temperature: 0.5
    top_p: 0.95
    no_of_cusine: 3
  intro: # light
    llm: yi-large-turbo
    # llm: yi-spark
    # llm: llama-3.1-70b
    # llm: gemini-1.5-flash
    temperature: 0.1
    top_p: 0.95
  need_for_recipe: # light
    llm: yi-large-turbo
    # llm: yi-spark
    # llm: llama-3.1-70b
    # llm: gemini-1.5-flash
    temperature: 0.5
    top_p: 0.95
  pick_ingredients: # heavy
    llm: yi-large
    # llm: llama-3.1-70b
    # llm: gemini-1.5-pro
    # llm: deepseek-chat
    temperature: 0.01
    top_p: 0.95
  need_for_cart: # light
    llm: yi-large-turbo
    # llm: yi-spark
    # llm: llama-3.1-70b
    # llm: gemini-1.5-flash
    temperature: 0.5
    top_p: 0.95
  write_recipe: # heavy - read target from large corpus
    llm: yi-large
    # llm: yi-spark
    # llm: llama-3.1-70b
    # llm: gemini-1.5-flash
    temperature: 0.1
    top_p: 0.95